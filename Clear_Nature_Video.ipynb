{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================\n",
        "# CLEAR NATURE\n",
        "# Detect, Segment and Inpaint\n",
        "# =================================================\n"
      ],
      "metadata": {
        "id": "J2xV5rTQHbAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1. INSTALLATIONS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"Installing dependencies compatible with Colab's Python 3.12...\")\n",
        "\n",
        "# Check current PyTorch version (Colab usually has it pre-installed)\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"Existing PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "except:\n",
        "    # Install PyTorch if not present (use latest stable for Python 3.12)\n",
        "    print(\"Installing PyTorch...\")\n",
        "    !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install Ultralytics YOLO (YOLOv8) for instance segmentation\n",
        "# Use latest version compatible with current PyTorch\n",
        "!pip install ultralytics\n",
        "\n",
        "# OpenCV is usually pre-installed in Colab, but ensure we have it\n",
        "!pip install opencv-python opencv-python-headless\n",
        "\n",
        "# Install core dependencies (use versions compatible with Python 3.12)\n",
        "!pip install Pillow scikit-image imageio imageio-ffmpeg\n",
        "\n",
        "# Clone E2FGVI repository\n",
        "if not os.path.exists('E2FGVI'):\n",
        "    !git clone https://github.com/MCG-NKU/E2FGVI.git\n",
        "    print(\"✓ E2FGVI repository cloned\")\n",
        "else:\n",
        "    print(\"✓ E2FGVI repository already exists\")\n",
        "\n",
        "%cd E2FGVI\n",
        "\n",
        "# Install mmcv-full (critical for E2FGVI - requires CUDA ops)\n",
        "# E2FGVI needs mmcv._ext with ModulatedDeformConv2d (deformable convolution)\n",
        "print(\"\\nInstalling mmcv-full with CUDA extensions...\")\n",
        "\n",
        "# Get current PyTorch and CUDA versions\n",
        "import torch\n",
        "torch_version = '.'.join(torch.__version__.split('.')[:2])  # e.g., \"2.5\"\n",
        "cuda_version = torch.version.cuda.replace('.', '')[:4]  # e.g., \"121\" from \"12.1\"\n",
        "\n",
        "print(f\"PyTorch version: {torch_version}\")\n",
        "print(f\"CUDA version: {cuda_version}\")\n",
        "\n",
        "# Uninstall any existing mmcv first\n",
        "!pip uninstall -y mmcv mmcv-full 2>/dev/null || true\n",
        "\n",
        "# Install mmcv-full from OpenMMLab pre-built wheels\n",
        "# This avoids compilation and should work with Python 3.12\n",
        "mmcv_url = f\"https://download.openmmlab.com/mmcv/dist/cu{cuda_version}/torch{torch_version}/index.html\"\n",
        "print(f\"Installing from: {mmcv_url}\")\n",
        "\n",
        "!pip install mmcv-full==1.7.2 -f {mmcv_url}\n",
        "\n",
        "# Verify installation\n",
        "print(\"\\nVerifying mmcv-full installation...\")\n",
        "try:\n",
        "    import mmcv\n",
        "    from mmcv.ops import ModulatedDeformConv2d\n",
        "    print(f\"✓ mmcv-full {mmcv.__version__} installed successfully\")\n",
        "    print(\"✓ mmcv.ops.ModulatedDeformConv2d available (required by E2FGVI)\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Import failed: {e}\")\n",
        "    print(\"\\n⚠️  mmcv-full installation unsuccessful.\")\n",
        "    print(\"Trying alternative: building from source (this will take 10-15 minutes)...\")\n",
        "\n",
        "    # Last resort: build from source\n",
        "    !pip install mmcv-full==1.7.2 --no-cache-dir\n",
        "\n",
        "    try:\n",
        "        import mmcv\n",
        "        from mmcv.ops import ModulatedDeformConv2d\n",
        "        print(f\"✓ mmcv-full {mmcv.__version__} built successfully\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"✗ Still failed: {e2}\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"CRITICAL ERROR: Unable to install mmcv-full\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"E2FGVI requires mmcv-full with CUDA extensions.\")\n",
        "        print(\"This is a known compatibility issue with newer Python/PyTorch versions.\")\n",
        "        print(\"\\nPossible solutions:\")\n",
        "        print(\"1. Use Python 3.10 or earlier (recommended)\")\n",
        "        print(\"2. Try a different Colab runtime\")\n",
        "        print(\"3. Use a local environment with compatible versions\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# Additional E2FGVI dependencies\n",
        "!pip install tensorboard tqdm pyyaml addict yapf\n",
        "\n",
        "# Check if E2FGVI needs patching for mmcv 2.x compatibility\n",
        "print(\"\\nChecking E2FGVI compatibility with mmcv 2.x...\")\n",
        "import_check = \"\"\"\n",
        "import sys\n",
        "sys.path.insert(0, '/content/E2FGVI')\n",
        "try:\n",
        "    from model.modules.flow_comp import SPyNet\n",
        "    print(\"✓ E2FGVI imports successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Import error: {e}\")\n",
        "    if \"mmcv.cnn\" in str(e):\n",
        "        print(\"  E2FGVI needs mmcv 1.x (mmcv-full). Applying compatibility fix...\")\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    exec(import_check)\n",
        "except:\n",
        "    print(\"  Running compatibility patch...\")\n",
        "    # Create a simple compatibility shim if needed\n",
        "    pass\n",
        "\n",
        "# Download E2FGVI-HQ pretrained model from correct URL\n",
        "!mkdir -p release_model\n",
        "\n",
        "# Check if model already exists\n",
        "if not os.path.exists('release_model/E2FGVI-HQ-CVPR22.pth'):\n",
        "    print(\"\\nDownloading E2FGVI-HQ model...\")\n",
        "    # Use alternative download methods\n",
        "    try:\n",
        "        # Try Google Drive link\n",
        "        !gdown --fuzzy \"https://drive.google.com/file/d/10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3/view?usp=sharing\" -O release_model/E2FGVI-HQ-CVPR22.pth\n",
        "    except:\n",
        "        # If gdown fails, try wget with direct link\n",
        "        !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3' -O release_model/E2FGVI-HQ-CVPR22.pth\n",
        "else:\n",
        "    print(\"✓ E2FGVI-HQ model already exists\")\n",
        "\n",
        "# download smp\n",
        "!pip install -q --no-cache-dir kagglehub segmentation-models-pytorch albumentations\n",
        "\n",
        "# Verify model downloaded correctly\n",
        "import os\n",
        "if os.path.exists('release_model/E2FGVI-HQ-CVPR22.pth'):\n",
        "    model_size = os.path.getsize('release_model/E2FGVI-HQ-CVPR22.pth') / (1024*1024)\n",
        "    print(f\"✓ E2FGVI-HQ model downloaded successfully ({model_size:.1f} MB)\")\n",
        "else:\n",
        "    print(\"✗ Model download failed! You may need to manually download from:\")\n",
        "    print(\"   https://drive.google.com/file/d/10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3/view\")\n",
        "\n",
        "%cd ..\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ ALL INSTALLATIONS COMPLETED!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Verify critical dependencies\n",
        "print(\"\\nVerifying installations...\")\n",
        "errors = []\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"✓ PyTorch {torch.__version__} (CUDA: {torch.cuda.is_available()})\")\n",
        "except ImportError:\n",
        "    errors.append(\"PyTorch not installed\")\n",
        "\n",
        "try:\n",
        "    import mmcv\n",
        "    print(f\"✓ mmcv {mmcv.__version__}\")\n",
        "except ImportError:\n",
        "    errors.append(\"mmcv not installed - E2FGVI will NOT work!\")\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(f\"✓ Ultralytics YOLO installed\")\n",
        "except ImportError:\n",
        "    errors.append(\"Ultralytics not installed\")\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"✓ OpenCV {cv2.__version__}\")\n",
        "except ImportError:\n",
        "    errors.append(\"OpenCV not installed\")\n",
        "\n",
        "try:\n",
        "  import segmentation_models_pytorch as smp\n",
        "  print(f\"✓ segmentation_models_pytorch installed\")\n",
        "except ImportError:\n",
        "    errors.append(\"segmentation_models_pytorch not installed\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"⚠️  INSTALLATION ERRORS DETECTED:\")\n",
        "    print(\"=\"*80)\n",
        "    for error in errors:\n",
        "        print(f\"  ✗ {error}\")\n",
        "    print(\"\\nPlease fix these errors before proceeding.\")\n",
        "    print(\"For mmcv issues, try: !pip install mmcv-full\")\n",
        "else:\n",
        "    print(\"\\n✓ All critical dependencies verified successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cao9T46mlLH-",
        "outputId": "ef8e4d03-f150-4582-a311-df195abb6fbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies compatible with Colab's Python 3.12...\n",
            "Existing PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.231-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.231-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.231 ultralytics-thop-2.0.18\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Cloning into 'E2FGVI'...\n",
            "remote: Enumerating objects: 345, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 345 (delta 51), reused 30 (delta 30), pack-reused 265 (from 1)\u001b[K\n",
            "Receiving objects: 100% (345/345), 36.75 MiB | 24.80 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "✓ E2FGVI repository cloned\n",
            "/content/E2FGVI\n",
            "\n",
            "Installing mmcv-full with CUDA extensions...\n",
            "PyTorch version: 2.9\n",
            "CUDA version: 126\n",
            "Installing from: https://download.openmmlab.com/mmcv/dist/cu126/torch2.9/index.html\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu126/torch2.9/index.html\n",
            "Collecting mmcv-full==1.7.2\n",
            "  Downloading mmcv-full-1.7.2.tar.gz (607 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from mmcv-full==1.7.2)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mmcv-full==1.7.2) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mmcv-full==1.7.2) (25.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mmcv-full==1.7.2) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from mmcv-full==1.7.2) (6.0.3)\n",
            "Collecting yapf (from mmcv-full==1.7.2)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf->mmcv-full==1.7.2) (4.5.0)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n",
            "  Building wheel for mmcv-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv-full: filename=mmcv_full-1.7.2-cp312-cp312-linux_x86_64.whl size=34527407 sha256=d4b77b34ab51fbf48f8c4113c00a577924f1e58196f296ad3e0f6d9558c3958a\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/bd/b0/d0828d8304578a5636cf0410137b95ac5bed23dda5f529d9e2\n",
            "Successfully built mmcv-full\n",
            "Installing collected packages: addict, yapf, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.7.2 yapf-0.43.0\n",
            "\n",
            "Verifying mmcv-full installation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ mmcv-full 1.7.2 installed successfully\n",
            "✓ mmcv.ops.ModulatedDeformConv2d available (required by E2FGVI)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.12/dist-packages (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.12/dist-packages (0.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "\n",
            "Checking E2FGVI compatibility with mmcv 2.x...\n",
            "✓ E2FGVI imports successfully\n",
            "\n",
            "Downloading E2FGVI-HQ model...\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3\n",
            "From (redirected): https://drive.google.com/uc?id=10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3&confirm=t&uuid=838b5304-977c-4757-af2a-8b1c9dfa6c1d\n",
            "To: /content/E2FGVI/release_model/E2FGVI-HQ-CVPR22.pth\n",
            "100% 165M/165M [00:03<00:00, 47.8MB/s]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ E2FGVI-HQ model downloaded successfully (156.9 MB)\n",
            "/content\n",
            "\n",
            "================================================================================\n",
            "✓ ALL INSTALLATIONS COMPLETED!\n",
            "================================================================================\n",
            "\n",
            "Verifying installations...\n",
            "✓ PyTorch 2.9.0+cu126 (CUDA: True)\n",
            "✓ mmcv 1.7.2\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "✓ Ultralytics YOLO installed\n",
            "✓ OpenCV 4.12.0\n",
            "✓ segmentation_models_pytorch installed\n",
            "\n",
            "✓ All critical dependencies verified successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "from PIL import Image\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add E2FGVI to path\n",
        "sys.path.append('E2FGVI')\n",
        "\n",
        "print(f\"\\nEnvironment Information:\")\n",
        "print(f\"  PyTorch version: {torch.__version__}\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"  OpenCV version: {cv2.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfsdy_tnJ4vR",
        "outputId": "096a1491-fb1f-4205-dff4-05eb6aa00a37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Environment Information:\n",
            "  PyTorch version: 2.9.0+cu126\n",
            "  CUDA available: True\n",
            "  CUDA version: 12.6\n",
            "  GPU: Tesla T4\n",
            "  OpenCV version: 4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. FUNCTIONS FOR CREATING FRAMES AND MASKS\n",
        "# ============================================================================\n",
        "\n",
        "def create_project_structure(video_path: str, base_output_dir: str = \"/content/output\") -> dict:\n",
        "    \"\"\"\n",
        "    Create organized folder structure for video processing.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video file\n",
        "        base_output_dir: Base directory for all outputs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing all relevant paths\n",
        "    \"\"\"\n",
        "    video_name = Path(video_path).stem\n",
        "    project_dir = os.path.join(base_output_dir, video_name)\n",
        "\n",
        "    paths = {\n",
        "        'project_dir': project_dir,\n",
        "        'frames_dir': os.path.join(project_dir, 'frames'),\n",
        "        'masks_dir': os.path.join(project_dir, 'masks'),\n",
        "        'yolo_masks_dir': os.path.join(project_dir, 'masks', 'yolo'),\n",
        "        'combined_masks_dir': os.path.join(project_dir, 'masks', 'combined'),\n",
        "        'results_dir': os.path.join(project_dir, 'results'),\n",
        "        'video_name': video_name,\n",
        "        'video_path': video_path\n",
        "    }\n",
        "\n",
        "    # Create all directories\n",
        "    for key, path in paths.items():\n",
        "        if key.endswith('_dir'):\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    return paths\n",
        "\n",
        "def extract_frames(video_path: str, frames_dir: str, fps: Optional[int] = None,\n",
        "                   target_resolution: Optional[Tuple[int, int]] = None) -> Tuple[int, int, int, float]:\n",
        "    \"\"\"\n",
        "    Extract frames from video as numbered PNG files (E2FGVI format: 00000.png, 00001.png, ...).\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video\n",
        "        frames_dir: Directory to save frames\n",
        "        fps: Target FPS (None = use original)\n",
        "        target_resolution: Target resolution as (width, height) tuple (None = use original)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (frame_count, width, height, original_fps)\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Determine output resolution\n",
        "    if target_resolution:\n",
        "        width, height = target_resolution\n",
        "        print(f\"  Original: {original_width}x{original_height}, Target: {width}x{height}, {original_fps:.2f} FPS, {total_frames} frames\")\n",
        "    else:\n",
        "        width, height = original_width, original_height\n",
        "        print(f\"  Video info: {width}x{height}, {original_fps:.2f} FPS, {total_frames} frames\")\n",
        "\n",
        "    # Calculate frame interval if FPS is specified\n",
        "    frame_interval = 1 if fps is None else int(original_fps / fps)\n",
        "\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save frame if it matches the interval\n",
        "        if frame_count % frame_interval == 0:\n",
        "            # Resize frame if target resolution specified\n",
        "            if target_resolution and (original_width != width or original_height != height):\n",
        "                frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # E2FGVI format: 5-digit zero-padded PNG files\n",
        "            frame_filename = os.path.join(frames_dir, f\"{saved_count:05d}.png\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % 100 == 0:\n",
        "            print(f\"  Processing frames: {frame_count}/{total_frames}\", end='\\r')\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"\\n  ✓ Extracted {saved_count} frames\")\n",
        "\n",
        "    return saved_count, width, height, original_fps\n",
        "\n",
        "\n",
        "def detect_and_segment_people(frames_dir: str, masks_dir: str,\n",
        "                               model_name: str = 'yolov8x-seg.pt',\n",
        "                               conf_threshold: float = 0.25) -> int:\n",
        "    \"\"\"\n",
        "    Detect and segment people using YOLO, creating binary masks.\n",
        "    E2FGVI requires: white (255) for regions to inpaint, black (0) for regions to keep.\n",
        "\n",
        "    Args:\n",
        "        frames_dir: Directory containing frame images\n",
        "        masks_dir: Directory to save mask images\n",
        "        model_name: YOLO model to use (yolov8n-seg.pt to yolov8x-seg.pt)\n",
        "        conf_threshold: Confidence threshold for detections\n",
        "\n",
        "    Returns:\n",
        "        Number of masks created\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Loading YOLO model: {model_name}\")\n",
        "    model = YOLO(model_name)\n",
        "\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.png')])\n",
        "    total_frames = len(frame_files)\n",
        "\n",
        "    print(f\"  Detecting people in {total_frames} frames...\")\n",
        "\n",
        "    for idx, frame_file in enumerate(frame_files):\n",
        "        frame_path = os.path.join(frames_dir, frame_file)\n",
        "        frame = cv2.imread(frame_path)\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        # Create blank mask (black background = keep these regions)\n",
        "        mask = np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "        # Run YOLO inference (class 0 = person in COCO dataset)\n",
        "        results = model(frame, conf=conf_threshold, classes=[0], verbose=False)\n",
        "\n",
        "        # Process segmentation masks\n",
        "        if results[0].masks is not None:\n",
        "            for seg_mask in results[0].masks.data:\n",
        "                # Resize mask to frame size\n",
        "                seg_mask_resized = cv2.resize(\n",
        "                    seg_mask.cpu().numpy(),\n",
        "                    (w, h),\n",
        "                    interpolation=cv2.INTER_LINEAR\n",
        "                )\n",
        "                # Add to combined mask (binary OR) - white (255) = inpaint these regions\n",
        "                mask = np.maximum(mask, (seg_mask_resized > 0.5).astype(np.uint8) * 255)\n",
        "\n",
        "        # Save mask with same filename as frame (E2FGVI requirement)\n",
        "        mask_path = os.path.join(masks_dir, frame_file)\n",
        "        cv2.imwrite(mask_path, mask)\n",
        "\n",
        "        if (idx + 1) % 50 == 0 or (idx + 1) == total_frames:\n",
        "            print(f\"  Processed: {idx + 1}/{total_frames} frames\", end='\\r')\n",
        "\n",
        "    print(f\"\\n  ✓ Created {total_frames} masks\")\n",
        "    return total_frames\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lXG9AhutWEQm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3.1 SPACE FOR ADDITIONAL MASK CREATION AND COMBINATION\n",
        "# ============================================================================\n",
        "\n",
        "# Define validation transform\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(256, 256), # Resize for model input\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # Normalize\n",
        "    ToTensorV2(), # Convert to PyTorch tensor\n",
        "])\n",
        "\n",
        "def combine_masks(mask_dirs: List[str], output_dir: str) -> int:\n",
        "    \"\"\"\n",
        "    Combine masks from multiple sources using OR operation.\n",
        "    Useful for combining YOLO masks with manual masks or other detection sources.\n",
        "\n",
        "    Args:\n",
        "        mask_dirs: List of directories containing masks\n",
        "        output_dir: Directory to save combined masks\n",
        "\n",
        "    Returns:\n",
        "        Number of combined masks created\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Combining masks from {len(mask_dirs)} sources...\")\n",
        "\n",
        "    # Get all mask files from first directory\n",
        "    mask_files = sorted([f for f in os.listdir(mask_dirs[0]) if f.endswith('.png')])\n",
        "\n",
        "    for mask_file in mask_files:\n",
        "        combined_mask = None\n",
        "\n",
        "        for mask_dir in mask_dirs:\n",
        "            mask_path = os.path.join(mask_dir, mask_file)\n",
        "            if os.path.exists(mask_path):\n",
        "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if combined_mask is None:\n",
        "                    combined_mask = mask\n",
        "                else:\n",
        "                    combined_mask = np.maximum(combined_mask, mask)\n",
        "\n",
        "        if combined_mask is not None:\n",
        "            output_path = os.path.join(output_dir, mask_file)\n",
        "            cv2.imwrite(output_path, combined_mask)\n",
        "\n",
        "    print(f\"  ✓ Combined {len(mask_files)} masks\")\n",
        "    return len(mask_files)\n",
        "\n",
        "\n",
        "def dilate_masks(input_dir: str, output_dir: str, kernel_size: int = 5, iterations: int = 1) -> int:\n",
        "    \"\"\"\n",
        "    Dilate masks to ensure complete coverage of people and remove edge artifacts.\n",
        "    This helps ensure no person pixels remain after inpainting.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing input masks\n",
        "        output_dir: Directory to save dilated masks\n",
        "        kernel_size: Size of dilation kernel (larger = more dilation)\n",
        "        iterations: Number of dilation iterations\n",
        "\n",
        "    Returns:\n",
        "        Number of masks processed\n",
        "    \"\"\"\n",
        "    print(f\"  Dilating masks (kernel={kernel_size}, iterations={iterations})...\")\n",
        "    mask_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.png')])\n",
        "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "\n",
        "    for mask_file in mask_files:\n",
        "        mask_path = os.path.join(input_dir, mask_file)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        dilated = cv2.dilate(mask, kernel, iterations=iterations)\n",
        "\n",
        "        output_path = os.path.join(output_dir, mask_file)\n",
        "        cv2.imwrite(output_path, dilated)\n",
        "\n",
        "    print(f\"  ✓ Dilated {len(mask_files)} masks\")\n",
        "    return len(mask_files)\n",
        "\n",
        "def setup_shadow_model():\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  shadow_model = smp.Unet(\n",
        "      encoder_name=\"resnet34\",        # Encoder backbone\n",
        "      encoder_weights=\"imagenet\",     # Pre-trained weights\n",
        "      in_channels=3,                  # RGB input\n",
        "      classes=1,                      # Binary segmentation (shadow/no-shadow)\n",
        "      activation=None,                # We'll apply sigmoid in loss\n",
        "  )\n",
        "\n",
        "  # Load entire model (simplest!)\n",
        "  shadow_model = torch.load('/content/shadow_model_full.pth', map_location=device, weights_only=False)\n",
        "  shadow_model.eval()\n",
        "\n",
        "  print(\"✓ Shadow Model loaded!\")\n",
        "  return shadow_model, device\n",
        "\n",
        "\n",
        "# define functions for use of model\n",
        "def predict_frame_shadow(image_rgb, model, device, threshold=0.5):\n",
        "    original_size = image_rgb.shape[:2]\n",
        "\n",
        "    # Transform\n",
        "    transformed = val_transform(image=image_rgb)\n",
        "    img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        output = torch.sigmoid(model(img_tensor))\n",
        "        pred = output.cpu().squeeze().numpy()\n",
        "\n",
        "    # Resize back to original size\n",
        "    pred = cv2.resize(pred, (original_size[1], original_size[0]))\n",
        "    mask = (pred > threshold).astype(np.uint8) * 255\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def filter_shadows_near_people(shadow_mask, people_mask, max_distance=50):\n",
        "    \"\"\"\n",
        "    Filter shadow mask to only keep shadows near people\n",
        "\n",
        "    Args:\n",
        "        shadow_mask: Binary shadow mask (H, W) with 255 for shadows\n",
        "        people_mask: Binary people mask (H, W) with 255 for people\n",
        "        max_distance: Maximum pixel distance to consider\n",
        "\n",
        "    Returns:\n",
        "        Filtered shadow mask\n",
        "    \"\"\"\n",
        "    shadow_binary = (shadow_mask > 127).astype(np.uint8)\n",
        "    people_binary = (people_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if people_binary.sum() == 0:\n",
        "        return np.zeros_like(shadow_mask)\n",
        "\n",
        "    kernel_size = max_distance * 2 + 1\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
        "    people_dilated = cv2.dilate(people_binary, kernel, iterations=1)\n",
        "\n",
        "    filtered_shadows = cv2.bitwise_and(shadow_binary, people_dilated)\n",
        "    return (filtered_shadows * 255).astype(np.uint8)\n",
        "\n",
        "def filter_shadows_connected_to_people(shadow_mask, people_mask):\n",
        "    \"\"\"\n",
        "    Filter shadows to only keep regions touching people (uses connected components)\n",
        "\n",
        "    Args:\n",
        "        shadow_mask: Binary shadow mask (H, W) with 255 for shadows\n",
        "        people_mask: Binary people mask (H, W) with 255 for people\n",
        "\n",
        "    Returns:\n",
        "        Filtered shadow mask with only shadows touching people\n",
        "    \"\"\"\n",
        "    shadow_binary = (shadow_mask > 127).astype(np.uint8)\n",
        "    people_binary = (people_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if people_binary.sum() == 0:\n",
        "        return np.zeros_like(shadow_mask)\n",
        "\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(shadow_binary, connectivity=8)\n",
        "    filtered_mask = np.zeros_like(shadow_binary)\n",
        "\n",
        "    for label in range(1, num_labels):\n",
        "        region_mask = (labels == label).astype(np.uint8)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        region_dilated = cv2.dilate(region_mask, kernel, iterations=1)\n",
        "\n",
        "        if cv2.bitwise_and(region_dilated, people_binary).sum() > 0:\n",
        "            filtered_mask = cv2.bitwise_or(filtered_mask, region_mask)\n",
        "\n",
        "    return (filtered_mask * 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "def detect_and_add_shadows(frames_dir: str,\n",
        "                           people_masks_dir: str,\n",
        "                           output_dir: str,\n",
        "                           shadow_model,\n",
        "                           device,\n",
        "                           val_transform,\n",
        "                           shadow_threshold: float = 0.5,\n",
        "                           max_distance: int = 50,\n",
        "                           shadow_coverage_threshold: float = 0.5) -> int:\n",
        "    \"\"\"\n",
        "    Detect shadows near people and add them to the masks.\n",
        "\n",
        "    Args:\n",
        "        frames_dir: Directory containing frame images\n",
        "        people_masks_dir: Directory containing people masks\n",
        "        output_dir: Directory to save combined masks (people + shadows)\n",
        "        shadow_model: Trained shadow detection model\n",
        "        device: PyTorch device (cuda/cpu)\n",
        "        val_transform: Transform function for shadow model input\n",
        "        shadow_threshold: Confidence threshold for shadow detection\n",
        "        max_distance: Maximum pixel distance from people to consider shadows\n",
        "        shadow_coverage_threshold: If shadow covers more than this % of frame, skip shadow filtering\n",
        "\n",
        "    Returns:\n",
        "        Number of masks processed\n",
        "    \"\"\"\n",
        "    print(f\"  Detecting shadows near people (max_distance={max_distance}px)...\")\n",
        "\n",
        "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.png')])\n",
        "    skipped_count = 0\n",
        "\n",
        "    for idx, frame_file in enumerate(frame_files):\n",
        "        # Read frame and people mask\n",
        "        frame_path = os.path.join(frames_dir, frame_file)\n",
        "        people_mask_path = os.path.join(people_masks_dir, frame_file)\n",
        "\n",
        "        frame = cv2.imread(frame_path)\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        people_mask = cv2.imread(people_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Predict shadow mask\n",
        "        original_size = frame_rgb.shape[:2]\n",
        "\n",
        "        # Transform\n",
        "        transformed = val_transform(image=frame_rgb)\n",
        "        img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            output = torch.sigmoid(shadow_model(img_tensor))\n",
        "            pred = output.cpu().squeeze().numpy()\n",
        "\n",
        "        # Resize back to original size\n",
        "        pred = cv2.resize(pred, (original_size[1], original_size[0]))\n",
        "        shadow_mask = (pred > shadow_threshold).astype(np.uint8) * 255\n",
        "\n",
        "        # Check shadow coverage - if too much shadow, skip filtering\n",
        "        shadow_binary = (shadow_mask > 127).astype(np.uint8)\n",
        "        total_pixels = shadow_binary.size\n",
        "        shadow_pixels = shadow_binary.sum()\n",
        "        shadow_coverage = shadow_pixels / total_pixels\n",
        "\n",
        "        if shadow_coverage >= shadow_coverage_threshold:\n",
        "            # Too much shadow in frame - just use people mask without shadow filtering\n",
        "            combined_mask = people_mask.copy()\n",
        "            skipped_count += 1\n",
        "        else:\n",
        "            # Two-stage shadow filtering\n",
        "            # Stage 1: Keep shadows near people (distance-based)\n",
        "            nearby_shadows = filter_shadows_near_people(shadow_mask, people_mask, max_distance)\n",
        "\n",
        "            # Stage 2: Keep shadows connected to people (component-based)\n",
        "            connected_shadows = filter_shadows_connected_to_people(shadow_mask, people_mask)\n",
        "\n",
        "            # Combine both filters (union)\n",
        "            filtered_shadows = cv2.bitwise_and(nearby_shadows, connected_shadows)\n",
        "\n",
        "            # Combine people mask with filtered shadows\n",
        "            combined_mask = cv2.bitwise_or(people_mask, filtered_shadows)\n",
        "\n",
        "        # Save combined mask\n",
        "        output_path = os.path.join(output_dir, frame_file)\n",
        "        cv2.imwrite(output_path, combined_mask)\n",
        "\n",
        "        if (idx + 1) % 50 == 0 or (idx + 1) == len(frame_files):\n",
        "            print(f\"  Processed: {idx + 1}/{len(frame_files)} frames\", end='\\r')\n",
        "\n",
        "    print(f\"\\n  ✓ Added shadows to {len(frame_files)} masks\")\n",
        "    if skipped_count > 0:\n",
        "        print(f\"  ℹ️  Skipped shadow filtering for {skipped_count} frames (>{shadow_coverage_threshold*100:.0f}% shadow coverage)\")\n",
        "    return len(frame_files)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iy52KlPvWEM6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. FUNCTIONS FOR USING E2FGVI\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_e2fgvi_input(frames_dir: str, masks_dir: str, video_name: str,\n",
        "                         e2fgvi_base_dir: str = \"E2FGVI\") -> dict:\n",
        "    \"\"\"\n",
        "    Prepare input structure for E2FGVI.\n",
        "    E2FGVI expects:\n",
        "      - inputs/<dataset_name>/<video_name>/frames/00000.png, 00001.png, ...\n",
        "      - inputs/<dataset_name>/<video_name>_masks/00000.png, 00001.png, ...\n",
        "\n",
        "    Args:\n",
        "        frames_dir: Directory containing frames\n",
        "        masks_dir: Directory containing masks\n",
        "        video_name: Name of the video\n",
        "        e2fgvi_base_dir: E2FGVI repository directory\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with E2FGVI input paths\n",
        "    \"\"\"\n",
        "    dataset_name = \"object_removal\"\n",
        "    e2fgvi_input_base = os.path.join(e2fgvi_base_dir, \"inputs\", dataset_name)\n",
        "\n",
        "    paths = {\n",
        "        'frames': os.path.join(e2fgvi_input_base, video_name, \"frames\"),\n",
        "        'masks': os.path.join(e2fgvi_input_base, f\"{video_name}_masks\")\n",
        "    }\n",
        "\n",
        "    # Create directories and copy files\n",
        "    for key, target_dir in paths.items():\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        # Remove existing files\n",
        "        for f in glob.glob(os.path.join(target_dir, \"*.png\")):\n",
        "            os.remove(f)\n",
        "\n",
        "        # Copy new files\n",
        "        source_dir = frames_dir if key == 'frames' else masks_dir\n",
        "        for img_file in sorted(glob.glob(os.path.join(source_dir, \"*.png\"))):\n",
        "            shutil.copy(img_file, target_dir)\n",
        "\n",
        "    print(f\"  ✓ Prepared E2FGVI input structure\")\n",
        "    return paths\n",
        "\n",
        "\n",
        "def run_e2fgvi_inference(video_name: str,\n",
        "                         width: int,\n",
        "                         height: int,\n",
        "                         output_path: str,\n",
        "                         e2fgvi_dir: str = \"E2FGVI\",\n",
        "                         use_hq_model: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Run E2FGVI inference for video inpainting.\n",
        "\n",
        "    Args:\n",
        "        video_name: Name of the video (used for input naming in E2FGVI)\n",
        "        width: Video width\n",
        "        height: Video height\n",
        "        output_path: Where to save the final output video\n",
        "        e2fgvi_dir: E2FGVI repository directory\n",
        "        use_hq_model: Whether to use HQ model (supports arbitrary resolution)\n",
        "\n",
        "    Returns:\n",
        "        Path to output video\n",
        "    \"\"\"\n",
        "    import re\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # Change to E2FGVI directory\n",
        "    original_dir = os.getcwd()\n",
        "    os.chdir(e2fgvi_dir)\n",
        "\n",
        "    try:\n",
        "        # Prepare paths for E2FGVI\n",
        "        dataset_name = \"object_removal\"\n",
        "        video_path = os.path.join(\"inputs\", dataset_name, video_name, \"frames\")\n",
        "        mask_path = os.path.join(\"inputs\", dataset_name, f\"{video_name}_masks\")\n",
        "\n",
        "        # Count total frames for progress bar\n",
        "        total_frames = len([f for f in os.listdir(video_path) if f.endswith('.png')])\n",
        "\n",
        "        # Prepare command\n",
        "        model_type = \"e2fgvi_hq\" if use_hq_model else \"e2fgvi\"\n",
        "        checkpoint = \"release_model/E2FGVI-HQ-CVPR22.pth\" if use_hq_model else \"release_model/E2FGVI-CVPR22.pth\"\n",
        "\n",
        "        cmd = [\n",
        "            \"python\", \"test.py\",\n",
        "            \"--model\", model_type,\n",
        "            \"--video\", video_path,\n",
        "            \"--mask\", mask_path,\n",
        "            \"--ckpt\", checkpoint,\n",
        "            \"--width\", str(width),\n",
        "            \"--height\", str(height),\n",
        "            \"--set_size\",  # Required flag when specifying width/height\n",
        "            '--num_ref', '40',\n",
        "            '--neighbor_stride', '5'\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n  Running E2FGVI inference...\")\n",
        "        print(f\"  Model: {model_type}\")\n",
        "        print(f\"  Resolution: {width}x{height}\")\n",
        "        print(f\"  Frames to process: {total_frames}\")\n",
        "        print(f\"  Video: {video_path}\")\n",
        "        print(f\"  Masks: {mask_path}\")\n",
        "\n",
        "        # Run inference with real-time progress monitoring\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "\n",
        "        # Create progress bar\n",
        "        pbar = tqdm(total=total_frames, desc=\"  Inpainting frames\", unit=\"frame\")\n",
        "\n",
        "        # Monitor output\n",
        "        output_lines = []\n",
        "        current_frame = 0\n",
        "\n",
        "        for line in process.stdout:\n",
        "            output_lines.append(line)\n",
        "\n",
        "            # Look for frame progress indicators in E2FGVI output\n",
        "            # E2FGVI might output something like \"Processing frame 10/100\" or similar\n",
        "            frame_match = re.search(r'(\\d+)/(\\d+)', line)\n",
        "            if frame_match:\n",
        "                frame_num = int(frame_match.group(1))\n",
        "                if frame_num > current_frame:\n",
        "                    pbar.update(frame_num - current_frame)\n",
        "                    current_frame = frame_num\n",
        "\n",
        "            # Alternative: look for progress percentage\n",
        "            percent_match = re.search(r'(\\d+)%', line)\n",
        "            if percent_match:\n",
        "                percent = int(percent_match.group(1))\n",
        "                target_frame = int(total_frames * percent / 100)\n",
        "                if target_frame > current_frame:\n",
        "                    pbar.update(target_frame - current_frame)\n",
        "                    current_frame = target_frame\n",
        "\n",
        "            # Show errors in real-time\n",
        "            if 'error' in line.lower() or 'traceback' in line.lower():\n",
        "                print(f\"\\n  {line.strip()}\")\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        # Complete the progress bar\n",
        "        if current_frame < total_frames:\n",
        "            pbar.update(total_frames - current_frame)\n",
        "        pbar.close()\n",
        "\n",
        "        if process.returncode != 0:\n",
        "            print(\"\\n  Full output:\")\n",
        "            print(''.join(output_lines))\n",
        "            raise RuntimeError(f\"E2FGVI inference failed with exit code {process.returncode}\")\n",
        "\n",
        "        print(\"  ✓ E2FGVI processing completed\")\n",
        "\n",
        "        # Find output video in E2FGVI results directory\n",
        "        # E2FGVI can create results in different locations depending on the test.py version:\n",
        "        # Option 1: results/<video_name>/<timestamp>.mp4\n",
        "        # Option 2: results/<timestamp>.mp4\n",
        "        # Option 3: results/object_removal/<video_name>.mp4\n",
        "\n",
        "        possible_result_dirs = [\n",
        "            os.path.join(\"/content/E2FGVI/results\"),\n",
        "            os.path.join(\"results\", video_name),           # Most common\n",
        "            os.path.join(\"results\"),                        # Sometimes directly in results\n",
        "            os.path.join(\"results\", \"object_removal\"),      # Alternative structure\n",
        "        ]\n",
        "\n",
        "        e2fgvi_output = None\n",
        "\n",
        "        # Search all possible locations\n",
        "        for result_dir in possible_result_dirs:\n",
        "            if not os.path.exists(result_dir):\n",
        "                continue\n",
        "\n",
        "            # Look for mp4 files\n",
        "            output_files = sorted([f for f in os.listdir(result_dir) if f.endswith('.mp4')])\n",
        "\n",
        "            if output_files:\n",
        "                # Get the most recent output\n",
        "                e2fgvi_output = os.path.join(result_dir, output_files[-1])\n",
        "                print(f\"  Found output: {e2fgvi_output}\")\n",
        "                break\n",
        "\n",
        "        if not e2fgvi_output:\n",
        "            # Debug: list all files in results directory\n",
        "            print(\"\\n  Debugging - Contents of results directory:\")\n",
        "            if os.path.exists(\"results\"):\n",
        "                for root, dirs, files in os.walk(\"results\"):\n",
        "                    print(f\"    {root}:\")\n",
        "                    for d in dirs:\n",
        "                        print(f\"      [DIR] {d}\")\n",
        "                    for f in files:\n",
        "                        print(f\"      [FILE] {f}\")\n",
        "            else:\n",
        "                print(\"    Results directory doesn't exist!\")\n",
        "\n",
        "            raise RuntimeError(f\"No output video found. Searched in: {possible_result_dirs}\")\n",
        "\n",
        "        # Copy to final destination\n",
        "        shutil.copy(e2fgvi_output, output_path)\n",
        "\n",
        "        print(f\"  ✓ Inpainting completed\")\n",
        "\n",
        "    finally:\n",
        "        # Return to original directory\n",
        "        os.chdir(original_dir)\n",
        "\n",
        "    return output_path\n"
      ],
      "metadata": {
        "id": "uA8n_gi-WEKQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. BATCH PROCESSING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def process_video(video_path: str,\n",
        "                  yolo_model: str = 'yolov8x-seg.pt',\n",
        "                  conf_threshold: float = 0.25,\n",
        "                  dilate_kernel: int = 5,\n",
        "                  dilate_iterations: int = 2,\n",
        "                  target_resolution: Optional[Tuple[int, int]] = None,\n",
        "                  max_frames: Optional[int] = None,\n",
        "                  detect_shadows: bool = False,\n",
        "                  shadow_model = None,\n",
        "                  shadow_device = None,\n",
        "                  shadow_transform = None,\n",
        "                  shadow_threshold: float = 0.5,\n",
        "                  shadow_max_distance: int = 50) -> Dict:\n",
        "    \"\"\"\n",
        "    Process a single video: extract frames, detect people, create masks, inpaint.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to video file\n",
        "        yolo_model: YOLO model to use\n",
        "        conf_threshold: Detection confidence threshold\n",
        "        dilate_kernel: Dilation kernel size\n",
        "        dilate_iterations: Number of dilation iterations\n",
        "        target_resolution: Target resolution as (width, height) tuple (None = use original)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with processing results and paths\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    video_name = Path(video_path).stem\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"PROCESSING: {video_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Create project structure\n",
        "        print(\"\\n[1/6] Creating project structure...\")\n",
        "        paths = create_project_structure(video_path)\n",
        "\n",
        "        # Extract frames\n",
        "        print(\"\\n[2/6] Extracting frames...\")\n",
        "        frame_count, width, height, fps = extract_frames(\n",
        "            video_path,\n",
        "            paths['frames_dir'],\n",
        "            target_resolution=target_resolution\n",
        "        )\n",
        "\n",
        "        # Detect and segment people\n",
        "        print(\"\\n[3/6] Detecting and segmenting people with YOLO...\")\n",
        "        mask_count = detect_and_segment_people(\n",
        "            paths['frames_dir'],\n",
        "            paths['yolo_masks_dir'],\n",
        "            model_name=yolo_model,\n",
        "            conf_threshold=conf_threshold\n",
        "        )\n",
        "\n",
        "        # Post-process masks (dilation for better coverage)\n",
        "        print(\"\\n[4/6] Post-processing masks...\")\n",
        "\n",
        "        # First, optionally add shadow detection\n",
        "        if detect_shadows:\n",
        "            if shadow_model is None or shadow_device is None or shadow_transform is None:\n",
        "                print(\"  ⚠️  Shadow detection enabled but model/device/transform not provided. Skipping shadows.\")\n",
        "                # Just copy YOLO masks to combined\n",
        "                for mask_file in os.listdir(paths['yolo_masks_dir']):\n",
        "                    shutil.copy(\n",
        "                        os.path.join(paths['yolo_masks_dir'], mask_file),\n",
        "                        os.path.join(paths['combined_masks_dir'], mask_file)\n",
        "                    )\n",
        "            else:\n",
        "                # Detect shadows and combine with people masks\n",
        "                detect_and_add_shadows(\n",
        "                    paths['frames_dir'],\n",
        "                    paths['yolo_masks_dir'],\n",
        "                    paths['combined_masks_dir'],\n",
        "                    shadow_model,\n",
        "                    shadow_device,\n",
        "                    shadow_transform,\n",
        "                    shadow_threshold=shadow_threshold,\n",
        "                    max_distance=shadow_max_distance\n",
        "                )\n",
        "        else:\n",
        "            # No shadow detection - just copy YOLO masks\n",
        "            for mask_file in os.listdir(paths['yolo_masks_dir']):\n",
        "                shutil.copy(\n",
        "                    os.path.join(paths['yolo_masks_dir'], mask_file),\n",
        "                    os.path.join(paths['combined_masks_dir'], mask_file)\n",
        "                )\n",
        "\n",
        "        # Then apply dilation\n",
        "        dilate_masks(\n",
        "            paths['combined_masks_dir'],\n",
        "            paths['combined_masks_dir'],  # Overwrite in place\n",
        "            kernel_size=dilate_kernel,\n",
        "            iterations=dilate_iterations\n",
        "        )\n",
        "\n",
        "\n",
        "        # Prepare E2FGVI input\n",
        "        print(\"\\n[5/6] Preparing E2FGVI input...\")\n",
        "        e2fgvi_paths = prepare_e2fgvi_input(\n",
        "            paths['frames_dir'],\n",
        "            paths['combined_masks_dir'],\n",
        "            video_name\n",
        "        )\n",
        "\n",
        "        # Run E2FGVI inpainting\n",
        "        print(\"\\n[6/6] Running E2FGVI inpainting...\")\n",
        "        output_video_path = os.path.join(paths['results_dir'], f\"{video_name}_inpainted.mp4\")\n",
        "        run_e2fgvi_inference(\n",
        "            video_name,\n",
        "            width,\n",
        "            height,\n",
        "            output_video_path\n",
        "        )\n",
        "\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"✓ SUCCESS: {video_name}\")\n",
        "        print(f\"  Output: {output_video_path}\")\n",
        "        print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'video_name': video_name,\n",
        "            'video_path': video_path,\n",
        "            'output_path': output_video_path,\n",
        "            'project_dir': paths['project_dir'],\n",
        "            'frames': frame_count,\n",
        "            'resolution': f\"{width}x{height}\",\n",
        "            'time': elapsed\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = (datetime.now() - start_time).total_seconds()\n",
        "        print(f\"\\n✗ ERROR processing {video_name}: {str(e)}\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'video_name': video_name,\n",
        "            'video_path': video_path,\n",
        "            'error': str(e),\n",
        "            'time': elapsed\n",
        "        }\n",
        "\n",
        "\n",
        "def batch_process_videos(input_dir: str = \"/content/\",\n",
        "                         yolo_model: str = 'yolov8x-seg.pt',\n",
        "                         conf_threshold: float = 0.25,\n",
        "                         dilate_kernel: int = 5,\n",
        "                         dilate_iterations: int = 2,\n",
        "                         target_resolution: Optional[Tuple[int, int]] = None,\n",
        "                         max_frames: Optional[int] = None,\n",
        "                         detect_shadows: bool = False,\n",
        "                         shadow_model = None,\n",
        "                         shadow_device = None,\n",
        "                         shadow_transform = None,\n",
        "                         shadow_threshold: float = 0.5,\n",
        "                         shadow_max_distance: int = 50) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Process all MP4 videos in the input directory.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing MP4 files\n",
        "        yolo_model: YOLO model to use\n",
        "        conf_threshold: Detection confidence threshold\n",
        "        dilate_kernel: Dilation kernel size\n",
        "        dilate_iterations: Number of dilation iterations\n",
        "        target_resolution: Target resolution as (width, height) tuple (None = use original)\n",
        "        max_frames: Maximum frames per video (None = no limit, helps avoid OOM)\n",
        "        detect_shadows: Whether to detect and include shadows in masks\n",
        "        shadow_model: Shadow detection model (required if detect_shadows=True)\n",
        "        shadow_device: PyTorch device for shadow model\n",
        "        shadow_transform: Transform function for shadow model\n",
        "        shadow_threshold: Confidence threshold for shadow detection\n",
        "        shadow_max_distance: Maximum distance from people to consider shadows\n",
        "\n",
        "    Returns:\n",
        "        List of result dictionaries for each video\n",
        "    \"\"\"\n",
        "    # Find all MP4 files\n",
        "    video_files = glob.glob(os.path.join(input_dir, \"*.mp4\"))\n",
        "\n",
        "    if not video_files:\n",
        "        print(f\"No MP4 files found in {input_dir}\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"BATCH PROCESSING: Found {len(video_files)} MP4 file(s)\")\n",
        "    print(\"=\"*80)\n",
        "    for vf in video_files:\n",
        "        print(f\"  - {Path(vf).name}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, video_path in enumerate(video_files, 1):\n",
        "        print(f\"\\n\\n{'='*80}\")\n",
        "        print(f\"VIDEO {idx}/{len(video_files)}\")\n",
        "        print('='*80)\n",
        "\n",
        "        result = process_video(\n",
        "            video_path,\n",
        "            yolo_model=yolo_model,\n",
        "            conf_threshold=conf_threshold,\n",
        "            dilate_kernel=dilate_kernel,\n",
        "            dilate_iterations=dilate_iterations,\n",
        "            target_resolution=target_resolution,\n",
        "            max_frames=max_frames,\n",
        "            detect_shadows=detect_shadows,\n",
        "            shadow_model=shadow_model,\n",
        "            shadow_device=shadow_device,\n",
        "            shadow_transform=shadow_transform,\n",
        "            shadow_threshold=shadow_threshold,\n",
        "            shadow_max_distance=shadow_max_distance\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "        # Clear GPU memory after each video to prevent accumulation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"  ✓ GPU memory cleared for next video\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"BATCH PROCESSING SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    successful = [r for r in results if r['success']]\n",
        "    failed = [r for r in results if not r['success'] and not r.get('skipped', False)]\n",
        "    skipped = [r for r in results if r.get('skipped', False)]\n",
        "\n",
        "    print(f\"\\nTotal videos: {len(results)}\")\n",
        "    print(f\"Successful: {len(successful)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "    if skipped:\n",
        "        print(f\"Skipped (too long): {len(skipped)}\")\n",
        "\n",
        "    if successful:\n",
        "        print(\"\\n✓ SUCCESSFUL VIDEOS:\")\n",
        "        for r in successful:\n",
        "            print(f\"  - {r['video_name']}\")\n",
        "            print(f\"    Output: {r['output_path']}\")\n",
        "            print(f\"    Time: {r['time']:.1f}s\")\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\n✗ FAILED VIDEOS:\")\n",
        "        for r in failed:\n",
        "            print(f\"  - {r['video_name']}\")\n",
        "            print(f\"    Error: {r['error']}\")\n",
        "\n",
        "    if skipped:\n",
        "        print(\"\\n⊘ SKIPPED VIDEOS (too long):\")\n",
        "        for r in skipped:\n",
        "            print(f\"  - {r['video_name']}\")\n",
        "            print(f\"    Reason: {r['error']}\")\n",
        "\n",
        "    total_time = sum(r['time'] for r in results)\n",
        "    print(f\"\\nTotal processing time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "t-eFLf88WEIK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6. RUN BATCH PROCESSING ON ALL MP4 FILES IN /content/\n",
        "# ============================================================================\n",
        "\n",
        "# Configuration\n",
        "INPUT_DIRECTORY = \"/content\"  # Process all MP4 files in this directory\n",
        "YOLO_MODEL = \"yolov8x-seg.pt\"  # Options: yolov8n-seg.pt (fastest), yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt (most accurate)\n",
        "CONF_THRESHOLD = 0.25          # Lower = detect more (more false positives), Higher = detect less (more false negatives)\n",
        "DILATE_KERNEL = 10              # Size of dilation kernel (larger = more mask expansion)\n",
        "DILATE_ITERATIONS = 2          # Number of dilation passes (more = larger masks)\n",
        "TARGET_RESOLUTION = (432, 240) # Resize videos to this resolution for faster processing (None = keep original)\n",
        "                               # Recommended for GPU memory: (432, 240) or (640, 360)\n",
        "                               # Higher resolutions may cause out-of-memory errors!\n",
        "MAX_FRAMES = 270\n",
        "# Shadow detection configuration\n",
        "DETECT_SHADOWS = True         # Set to True to detect and remove shadows along with people\n",
        "SHADOW_THRESHOLD = 0.5         # Confidence threshold for shadow detection (0.0-1.0)\n",
        "SHADOW_MAX_DISTANCE = 100       # Maximum pixel distance from people to consider shadows\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Input directory: {INPUT_DIRECTORY}\")\n",
        "print(f\"YOLO model: {YOLO_MODEL}\")\n",
        "print(f\"Confidence threshold: {CONF_THRESHOLD}\")\n",
        "print(f\"Mask dilation: kernel={DILATE_KERNEL}, iterations={DILATE_ITERATIONS}\")\n",
        "print(f\"Target resolution: {TARGET_RESOLUTION if TARGET_RESOLUTION else 'Original (no rescaling)'}\")\n",
        "print(f\"Shadow detection: {'Enabled' if DETECT_SHADOWS else 'Disabled'}\")\n",
        "if DETECT_SHADOWS:\n",
        "    print(f\"  Shadow threshold: {SHADOW_THRESHOLD}\")\n",
        "    print(f\"  Shadow max distance: {SHADOW_MAX_DISTANCE}px\")\n",
        "\n",
        "# Clear GPU memory before starting\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB total\")\n",
        "    print(\"✓ GPU cache cleared\")\n",
        "\n",
        "# Set PyTorch memory optimization\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "print(\"✓ GPU memory fragmentation optimization enabled\")\n",
        "\n",
        "shadow_model = None\n",
        "shadow_device = None\n",
        "shadow_transform = val_transform\n",
        "# Load Shadow\n",
        "if DETECT_SHADOWS:\n",
        "  shadow_model, shadow_device = setup_shadow_model()\n",
        "\n",
        "# Run batch processing\n",
        "results = batch_process_videos(\n",
        "    input_dir=INPUT_DIRECTORY,\n",
        "    yolo_model=YOLO_MODEL,\n",
        "    conf_threshold=CONF_THRESHOLD,\n",
        "    dilate_kernel=DILATE_KERNEL,\n",
        "    dilate_iterations=DILATE_ITERATIONS,\n",
        "    target_resolution=TARGET_RESOLUTION,\n",
        "    max_frames=MAX_FRAMES,\n",
        "    detect_shadows=DETECT_SHADOWS,\n",
        "    shadow_model=shadow_model if DETECT_SHADOWS else None,\n",
        "    shadow_device=shadow_device if DETECT_SHADOWS else None,\n",
        "    shadow_transform=shadow_transform if DETECT_SHADOWS else None,\n",
        "    shadow_threshold=SHADOW_THRESHOLD,\n",
        "    shadow_max_distance=SHADOW_MAX_DISTANCE\n",
        ")\n",
        "\n",
        "# Display first successful result (if any) in Colab\n",
        "successful_results = [r for r in results if r['success']]\n",
        "if successful_results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DISPLAYING FIRST RESULT\")\n",
        "    print(\"=\"*80)\n",
        "    from IPython.display import Video, display\n",
        "    first_result = successful_results[0]\n",
        "    print(f\"Video: {first_result['video_name']}\")\n",
        "    display(Video(first_result['output_path'], width=640))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RQBVGk0GWEFM",
        "outputId": "73126831-eba5-4f24-c303-6ee6f1167358"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CONFIGURATION\n",
            "================================================================================\n",
            "Input directory: /content\n",
            "YOLO model: yolov8x-seg.pt\n",
            "Confidence threshold: 0.25\n",
            "Mask dilation: kernel=10, iterations=2\n",
            "Target resolution: (432, 240)\n",
            "Shadow detection: Enabled\n",
            "  Shadow threshold: 0.5\n",
            "  Shadow max distance: 100px\n",
            "\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB total\n",
            "✓ GPU cache cleared\n",
            "✓ GPU memory fragmentation optimization enabled\n",
            "✓ Shadow Model loaded!\n",
            "\n",
            "================================================================================\n",
            "BATCH PROCESSING: Found 3 MP4 file(s)\n",
            "================================================================================\n",
            "  - shadow concrete.mp4\n",
            "  - shadow wall.mp4\n",
            "  - shadow bench.mp4\n",
            "\n",
            "\n",
            "================================================================================\n",
            "VIDEO 1/3\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROCESSING: shadow concrete\n",
            "================================================================================\n",
            "\n",
            "[1/6] Creating project structure...\n",
            "\n",
            "[2/6] Extracting frames...\n",
            "  Original: 848x478, Target: 432x240, 30.00 FPS, 146 frames\n",
            "\n",
            "  ✓ Extracted 146 frames\n",
            "\n",
            "[3/6] Detecting and segmenting people with YOLO...\n",
            "\n",
            "  Loading YOLO model: yolov8x-seg.pt\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x-seg.pt to 'yolov8x-seg.pt': 100% ━━━━━━━━━━━━ 137.4MB 93.3MB/s 1.5s\n",
            "  Detecting people in 146 frames...\n",
            "  Processed: 146/146 frames\n",
            "  ✓ Created 146 masks\n",
            "\n",
            "[4/6] Post-processing masks...\n",
            "  Detecting shadows near people (max_distance=100px)...\n",
            "  Processed: 146/146 frames\n",
            "  ✓ Added shadows to 146 masks\n",
            "  Dilating masks (kernel=10, iterations=2)...\n",
            "  ✓ Dilated 146 masks\n",
            "\n",
            "[5/6] Preparing E2FGVI input...\n",
            "  ✓ Prepared E2FGVI input structure\n",
            "\n",
            "[6/6] Running E2FGVI inpainting...\n",
            "\n",
            "  Running E2FGVI inference...\n",
            "  Model: e2fgvi_hq\n",
            "  Resolution: 432x240\n",
            "  Frames to process: 146\n",
            "  Video: inputs/object_removal/shadow concrete/frames\n",
            "  Masks: inputs/object_removal/shadow concrete_masks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Inpainting frames: 100%|██████████| 146/146 [01:12<00:00,  2.01frame/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ E2FGVI processing completed\n",
            "  Found output: /content/E2FGVI/results/frames_results.mp4\n",
            "  ✓ Inpainting completed\n",
            "\n",
            "================================================================================\n",
            "✓ SUCCESS: shadow concrete\n",
            "  Output: /content/output/shadow concrete/results/shadow concrete_inpainted.mp4\n",
            "  Time: 100.1s (1.7 minutes)\n",
            "================================================================================\n",
            "  ✓ GPU memory cleared for next video\n",
            "\n",
            "\n",
            "================================================================================\n",
            "VIDEO 2/3\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROCESSING: shadow wall\n",
            "================================================================================\n",
            "\n",
            "[1/6] Creating project structure...\n",
            "\n",
            "[2/6] Extracting frames...\n",
            "  Original: 848x478, Target: 432x240, 30.00 FPS, 147 frames\n",
            "\n",
            "  ✓ Extracted 147 frames\n",
            "\n",
            "[3/6] Detecting and segmenting people with YOLO...\n",
            "\n",
            "  Loading YOLO model: yolov8x-seg.pt\n",
            "  Detecting people in 147 frames...\n",
            "  Processed: 147/147 frames\n",
            "  ✓ Created 147 masks\n",
            "\n",
            "[4/6] Post-processing masks...\n",
            "  Detecting shadows near people (max_distance=100px)...\n",
            "  Processed: 147/147 frames\n",
            "  ✓ Added shadows to 147 masks\n",
            "  Dilating masks (kernel=10, iterations=2)...\n",
            "  ✓ Dilated 147 masks\n",
            "\n",
            "[5/6] Preparing E2FGVI input...\n",
            "  ✓ Prepared E2FGVI input structure\n",
            "\n",
            "[6/6] Running E2FGVI inpainting...\n",
            "\n",
            "  Running E2FGVI inference...\n",
            "  Model: e2fgvi_hq\n",
            "  Resolution: 432x240\n",
            "  Frames to process: 147\n",
            "  Video: inputs/object_removal/shadow wall/frames\n",
            "  Masks: inputs/object_removal/shadow wall_masks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Inpainting frames: 100%|██████████| 147/147 [01:11<00:00,  2.05frame/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ E2FGVI processing completed\n",
            "  Found output: /content/E2FGVI/results/frames_results.mp4\n",
            "  ✓ Inpainting completed\n",
            "\n",
            "================================================================================\n",
            "✓ SUCCESS: shadow wall\n",
            "  Output: /content/output/shadow wall/results/shadow wall_inpainted.mp4\n",
            "  Time: 92.6s (1.5 minutes)\n",
            "================================================================================\n",
            "  ✓ GPU memory cleared for next video\n",
            "\n",
            "\n",
            "================================================================================\n",
            "VIDEO 3/3\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROCESSING: shadow bench\n",
            "================================================================================\n",
            "\n",
            "[1/6] Creating project structure...\n",
            "\n",
            "[2/6] Extracting frames...\n",
            "  Original: 848x478, Target: 432x240, 29.76 FPS, 128 frames\n",
            "\n",
            "  ✓ Extracted 128 frames\n",
            "\n",
            "[3/6] Detecting and segmenting people with YOLO...\n",
            "\n",
            "  Loading YOLO model: yolov8x-seg.pt\n",
            "  Detecting people in 128 frames...\n",
            "  Processed: 128/128 frames\n",
            "  ✓ Created 128 masks\n",
            "\n",
            "[4/6] Post-processing masks...\n",
            "  Detecting shadows near people (max_distance=100px)...\n",
            "  Processed: 128/128 frames\n",
            "  ✓ Added shadows to 128 masks\n",
            "  Dilating masks (kernel=10, iterations=2)...\n",
            "  ✓ Dilated 128 masks\n",
            "\n",
            "[5/6] Preparing E2FGVI input...\n",
            "  ✓ Prepared E2FGVI input structure\n",
            "\n",
            "[6/6] Running E2FGVI inpainting...\n",
            "\n",
            "  Running E2FGVI inference...\n",
            "  Model: e2fgvi_hq\n",
            "  Resolution: 432x240\n",
            "  Frames to process: 128\n",
            "  Video: inputs/object_removal/shadow bench/frames\n",
            "  Masks: inputs/object_removal/shadow bench_masks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Inpainting frames: 100%|██████████| 128/128 [00:58<00:00,  2.19frame/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ E2FGVI processing completed\n",
            "  Found output: /content/E2FGVI/results/frames_results.mp4\n",
            "  ✓ Inpainting completed\n",
            "\n",
            "================================================================================\n",
            "✓ SUCCESS: shadow bench\n",
            "  Output: /content/output/shadow bench/results/shadow bench_inpainted.mp4\n",
            "  Time: 78.9s (1.3 minutes)\n",
            "================================================================================\n",
            "  ✓ GPU memory cleared for next video\n",
            "\n",
            "\n",
            "================================================================================\n",
            "BATCH PROCESSING SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Total videos: 3\n",
            "Successful: 3\n",
            "Failed: 0\n",
            "\n",
            "✓ SUCCESSFUL VIDEOS:\n",
            "  - shadow concrete\n",
            "    Output: /content/output/shadow concrete/results/shadow concrete_inpainted.mp4\n",
            "    Time: 100.1s\n",
            "  - shadow wall\n",
            "    Output: /content/output/shadow wall/results/shadow wall_inpainted.mp4\n",
            "    Time: 92.6s\n",
            "  - shadow bench\n",
            "    Output: /content/output/shadow bench/results/shadow bench_inpainted.mp4\n",
            "    Time: 78.9s\n",
            "\n",
            "Total processing time: 271.6s (4.5 minutes)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "DISPLAYING FIRST RESULT\n",
            "================================================================================\n",
            "Video: shadow concrete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video src=\"/content/output/shadow concrete/results/shadow concrete_inpainted.mp4\" controls  width=\"640\" >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cks92ex_WEC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZLCIPJC2WEAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F188Uln6WD9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1Gjd_NIQrCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "en7fx0klHbTe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}